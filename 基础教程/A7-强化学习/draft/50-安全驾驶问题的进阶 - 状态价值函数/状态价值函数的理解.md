
## 状态价值函数的理解


在醉汉回家的例子中，奖励函数可以有几种设计方式：

1. 对状态奖励：


```Python
class States(Enum):
    Start = 0
    A = 1
    B = 2
    C = 3
    D = 4
    Home = 5
    End = 6

P = np.array(
    [  # S    A    B    C    D    H    end   
        [0.5, 0.5, 0,   0,   0  , 0  , 0  ], # S
        [0.5, 0,   0.5, 0,   0,   0,   0  ], # A
        [0,   0.5, 0,   0.5, 0,   0,   0  ], # B
        [0,   0,   0.5, 0,   0.5, 0,   0  ], # C
        [0,   0,   0,   0.5, 0,   0.5, 0  ], # D
        [0,   0,   0,   0,   0,   0,   1.0], # Home
        [0,   0,   0,   0,   0,   0,   1.0], # end
])
```

### 状态奖励方法

#### 第一种情况

只有到达家的状态奖励 +1，其它状态都是 0。这样可以达到的效果是，一旦智能体跌跌撞撞地达到了一次家的状态，可以得到 +1 的奖励，它就会尝到甜头并牢记，在后面的学习中会不断强化这种记忆和过程。奖励值如图 1 所示。

<center>
<img src="./img/RandomWalker-1.png">

图 1 醉汉回家
</center>

```Python
# 状态奖励值
#     S, A, B, C, D, H, e
R1 = [0, 0, 0, 0, 0, 1, 0]
```

结果

|状态$\to$|S|A|B|C|D|H|E|
|-|-|-|-|-|-|-|-|
|$\gamma=1$|1.0|1.0|1.0|1.0|1.0|1.0|0.0|
|$\gamma=0.9$|0.16|0.19|0.27|0.41|0.63|1.0|0.0|

从结果可以看到，当折扣因子 $\gamma=1$ 时，无折扣，根据公式
$$
V(s)=\mathbb E [G_t|S_t=s]
=\mathbb E [ R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+ \cdots + \gamma^{T-t-1} R_T]
\tag{1}
$$

所有的状态函数值都会是最后到家的奖励值 $R_T=1$，也就无法区分各个状态的好坏了。但是当 $\gamma=0.9$ 时，由于折扣的存在，距离终止状态越近的状态，具有的状态函数值越大，这也是理所当然的。

所以，折扣值的存在对于奖励机制很有用，当不能确定奖励值是否合理时，一定要令折扣值小于 1，否则没有研究价值。

#### 第二种情况

既然第一种奖励值设计有缺陷，我们把所有的奖励值都减 1，会不会是同样的效果呢？如图 2 所示。

<center>
<img src="./img/RandomWalker-2.png">

图 2 醉汉回家
</center>

```
# 状态奖励值
#     S,   A,  B,  C,  D, H, End
R2 = [-1, -1, -1, -1, -1, 0, 0]
```

结果

表 2

|状态$\to$|S|A|B|C|D|H|E|
|-|-|-|-|-|-|-|-|
|$\gamma=1$|-30|-28|-24|-18|-10|0.0|0.0|
|$\gamma=0.9$|-8.4|-8.1|-7.3|-5.9|-3.7|0.0|0.0|

情况发生了变化，状态值并不是在第一种情况的基础上普遍减 1，而是发生了显著的变化，这是因为 $R=-1$ 的不断累加使得远离终止状态的值变得非常小。

有的读者可能会问：起始状态 S 的的状态值为什么不是 -5 而是 -30？因为根据式 1，状态函数值是 $G$ 的期望，$G$ 代表了状态变化序列，由于在每个状态上都有 0.5 的概率返回上一个状态，所以状态变化序列（即马尔科夫链）会变得非常曲折，就会让 -1 这个值累加得越来越大。

当有折扣存在时，这种累加效果会减弱很多，所以当 $\gamma=0.9$ 时，各个状态的函数值的差别会变得比较平缓。

### 过程奖励方法

下面我们研究一下过程奖励方法对状态函数值的影响。

#### 第一种情况

假设每走一步都有 -1 的“奖励”，实际上是惩罚，意在鼓励智能体（一个还没有接受强化训练的智能体和醉汉没什么区别）尽快找到回家的路，最后累计的负值越小越好。如图 3 所示。

<center>
<img src="./img/RandomWalker-3.png">

图 3 醉汉回家
</center>



```Python
# 过程奖励值
R3 = [
    (-1)*0.5+(-1)*0.5,  # S
    (-1)*0.5+(-1)*0.5,  # A
    (-1)*0.5+(-1)*0.5,  # B
    (-1)*0.5+(-1)*0.5,  # C
    (-1)*0.5+(-1)*0.5,  # D
    0*1.0,              # Home
    0,                  # End
]
```

|状态$\to$|S|A|B|C|D|H|E|
|-|-|-|-|-|-|-|-|
|$\gamma=1$|-30|-28|-24|-18|-10|0.0|0.0|
|$\gamma=0.9$|-8.435|-8.088|-7.315|-5.946|-3.676|0.0|0.0|

#### 值二

<center>
<img src="./img/RandomWalker-4.png">

图 4 醉汉回家
</center>

```Python
# 过程奖励值
R4 = [
    (-1)*0.5+(-1)*0.5,  # S
    (-1)*0.5+(-1)*0.5,  # A
    (-1)*0.5+(-1)*0.5,  # B
    (-1)*0.5+(-1)*0.5,  # C
    (-1)*0.5+(-1)*0.5,  # D
    1*1.0,              # Home
    0,                  # End
]
```

|状态$\to$|S|A|B|C|D|H|E|
|-|-|-|-|-|-|-|-|
|$\gamma=1$|-29|-27|-23|-17|-9|0.0|0.0|
|$\gamma=0.9$|-8.279|-7.897|-7.047|-5.54|-3.043|1.0|0.0|